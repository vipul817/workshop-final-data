{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4edd4c",
   "metadata": {},
   "source": [
    "# Welcome to the AWS Worshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01bf310",
   "metadata": {},
   "source": [
    "**Brief description about the dataset**\n",
    "\n",
    "Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "We will explore this dataset and find out factors that contribute the most for diabetes causation. We will also build Machine Learning Models that can help to predict whether a person is diabetic or not and try to improve the model by performing Cross Validation and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c29879",
   "metadata": {},
   "source": [
    "### Steps to be followed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa889f",
   "metadata": {},
   "source": [
    "* Importing necessary libraries\n",
    "* Creating s3 bucket\n",
    "* Importing and exporting the data from git repository and s3 bucket.\n",
    "* Data preprosessing\n",
    "* Exploratory data analysis\n",
    "* Building and deploying the model\n",
    "* Prediction\n",
    "* Deleting the endpoints and s3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1007683",
   "metadata": {},
   "source": [
    "### Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4399b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic analysis library\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58be4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker library\n",
    "\n",
    "import sagemaker                                             #Build in algorithms that are present in sagemaker                                             \n",
    "import boto3                                                 #Allows to create, update and delete aws resources from s3          \n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri  #Downloading image container of the models \n",
    "from sagemaker.session import s3_input, Session              #Provides convenient methods for manipulating entities and resouces that amazon sagemaker uses, such as training jobs, endpoints and input datasets in s3.\n",
    "from sagemaker import get_execution_role                     #IAM role created for the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379a9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization libraries\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade boto3   # To avoid getting errors while import or exporting data in s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a070d6",
   "metadata": {},
   "source": [
    "### Creating  S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82823df5",
   "metadata": {},
   "source": [
    "The s3 bucket can also be created manually by going to the s3 management console and clicking on create bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'awsworkshop301' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')  #To get the access of s3 bucket\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75401a4f",
   "metadata": {},
   "source": [
    "**AWS Identity and Access Management(IAM)** roles are entities you create and assign specific permissions to that allow trusted identities such as workforce identities and applications to perform actions in AWS. When your trusted identities assume IAM roles, they are granted only the permissions scoped by those IAM roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79372e9a",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b16d22",
   "metadata": {},
   "source": [
    "We are using the diabetes dataset and it is divided into two sections. The first part of the dataset will be imported from git repository and the second part will be imported from the s3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466416e5",
   "metadata": {},
   "source": [
    "#### Importing first part of the data from git repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43716a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_csv('diabetes_first_data.csv')\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e317f8",
   "metadata": {},
   "source": [
    "#### Importing second part of the data from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading the second data into s3\n",
    "\n",
    "s3=boto3.resource('s3')\n",
    "s3.meta.client.upload_file('diabetes_second_data.csv',bucket_name,'diabetes_second_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset from s3\n",
    "\n",
    "role=get_execution_role()\n",
    "data_key='diabetes_second_data.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "\n",
    "data2=pd.read_csv(data_location)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a8838",
   "metadata": {},
   "source": [
    "#### Merging the datasets to get a complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcfabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data=pd.merge(data1,data2, on='Test ID')\n",
    "merge_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95182854",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the test id as it is not necessary for exploratory data analysis.\n",
    "\n",
    "df=merge_data.drop('Test ID', axis='columns')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9df339",
   "metadata": {},
   "source": [
    "**'Pregnancies'** is the number of pregnancies to date.\n",
    "\n",
    "**'Glucose'** is the plasma glucose concentration over 2 hours in an oral glucose tolerance test.\n",
    "\n",
    "**'BloodPressure'** is the diastolic blood pressure, measured in millimeters of mercury (mm Hg).\n",
    "\n",
    "**'SkinThickness'** is the triceps skin fold thickness, measured in millimeters (mm).\n",
    "\n",
    "**'Insulin'** is the 2-hour serum insulin, measured in micrometre units per millilitre (mu U/ml).\n",
    "\n",
    "**'BMI'** is the body mass index (BMI) for weight in kg and height in m (kg/m^2).\n",
    "\n",
    "**'DiabetesPedigreeFunction'** is a function that scores likelihood of diabetes based on family history, with a realistic range of 0.08 to 2.42.\n",
    "\n",
    "**'Age'** of a person in years.\n",
    "\n",
    "**'Outcome'** is the target class label, where 0 represents absence and 1 represents presence of diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the number of entries, the names of the column attributes, the data type and the memory space used\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11ecb3",
   "metadata": {},
   "source": [
    "The dataset contains 768 rows of records and 9 columns of attributes. The data types of the attributes consist of 6 quantitative discrete numerical integers and 2 quantitative continuous numerical float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ce071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of the attributes, including measures of central tendency and measures of dispersion\n",
    "\n",
    "ab=df.describe() \n",
    "ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coverting the above table into a dataframe and uploading it into the s3 bucket.\n",
    "\n",
    "export_dataframe=pd.DataFrame(ab)\n",
    "describe_key='describe.xlsx'\n",
    "describe_location='s3://{}/{}'.format(bucket_name, describe_key)\n",
    "export_dataframe.to_excel(describe_location)\n",
    "\n",
    "#We can download this file from s3 bucket directly in our local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb327ce2",
   "metadata": {},
   "source": [
    "**Q) For the Iris dataset in sklearn find the summary statistics and export it into the s3 bucket created manually in the form of an excel file.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09f504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd241fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e626386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris_data=pd.DataFrame(datasets.load_iris().data)\n",
    "iris_data.columns=datasets.load_iris().feature_names\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values \n",
    "\n",
    "df.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate rows\n",
    "\n",
    "duplicated_rows = df[df.duplicated()]\n",
    "duplicated_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a95bb",
   "metadata": {},
   "source": [
    "There are no duplications in the dataset.\n",
    "\n",
    "Duplicated rows or records will not be dropped from the dataset in this case. There is no certain redundancy which causes inaccurate results and outcomes, since the dataset has no unique identfier that denotes separate entities. Despite this, the dataset will still be checked for duplicated rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82920049",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c73264",
   "metadata": {},
   "source": [
    "EDA aims to perform initial investigations on data before formal modeling and graphical representations and visualisations, in order to discover patterns, look over assumptions, and test hypothesis. The summarised information on main characteristics and hidden trends in data can help the doctor to identify concern areas of problems and the resolution of these can boost their accuracy in diagnosing diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the outcome labels\n",
    "\n",
    "df['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae71964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the outcome col. histogram\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.countplot(data=df, x='Outcome',palette=\"autumn\",facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\n",
    "plt.savefig(\"countplot.jpg\")  #saving the image of this plot in the sagemaker console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the image of this plot directly into the s3 bucket\n",
    "\n",
    "s3=boto3.resource('s3')\n",
    "s3.meta.client.upload_file('countplot.jpg',bucket_name,'countplot.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae171e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a piechart to get the percentage of diabetic and non-diabetic population\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "labels = ['Diabetic', \n",
    "         'Non-Diabetic']\n",
    "percentages = [34.89, 65.10]\n",
    "explode=(0.1,0)\n",
    "ax.pie(percentages, explode=explode, labels=labels, autopct='%1.0f%%', \n",
    "       shadow=False, startangle=0,   \n",
    "       pctdistance=1.2,labeldistance=1.4)\n",
    "ax.legend(frameon=False, bbox_to_anchor=(1.5,0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd11ed",
   "metadata": {},
   "source": [
    "**Q) Save the image of the pie chart and export it to the s3 bucket created manually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf77c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951fca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a590e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution of all features\n",
    "\n",
    "df.hist(figsize=(12,10),grid=False)\n",
    "sns.set_style('white')\n",
    "plt.savefig(\"freqdist.jpg\")    #saving the image of this plot in the sagemaker console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d3098e",
   "metadata": {},
   "source": [
    "The Histograms provide us a more or less clear picture that the Attributes are positively skewed.\n",
    "\n",
    "Furthermore, the histogram density plots and their respective highest point in the curves show the patterns that diabetes patients generally have higher numbers of Pregnancies, higher Glucose and BMI readings, and older in Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a909101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the image of these plots directly into the s3 bucket\n",
    "\n",
    "s3=boto3.resource('s3')\n",
    "s3.meta.client.upload_file('freqdist.jpg',bucket_name,'freqdist.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ff89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we would know what is the effect of Age on the Outcome because we have heard that as the age increases, the chances of diabetes also commonly increases.\n",
    "\n",
    "sns.boxplot(x = 'Outcome', y = 'Age', data = df)\n",
    "plt.title('Age vs Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae3caf",
   "metadata": {},
   "source": [
    "Yes, we were right, the median of the age of diabetic people is greater than that of non-diabetic people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c80ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's also check the effect of Blood Pressure on the Outcome.\n",
    "\n",
    "sns.boxplot(x = 'Outcome', y = 'BloodPressure', data = df, palette = 'Blues')\n",
    "plt.title('BP vs Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d9ccf",
   "metadata": {},
   "source": [
    "The median of the BloodPressure of diabetic people lies close to the 75th Percentile of non-diabetic people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c24573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One would also want to know the chances of getting diabetes, if it is common in the family. We can check that with the Diabetes Pedigree Functio\n",
    "\n",
    "my_pal = {0: \"lightgreen\", 1: \"lightblue\"}\n",
    "sns.boxplot(x = 'Outcome', y = 'DiabetesPedigreeFunction', data = df, palette = my_pal)\n",
    "plt.title('DPF vs Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859969c",
   "metadata": {},
   "source": [
    "Quite a proportion of people having high DPF does not end up having Diabetes. But usually the diabetic people have DPF value close to 0.5 (50th Percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed4359",
   "metadata": {},
   "source": [
    "#### Gluscose Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56505d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pal = {0: \"lightgrey\", 1: \"lightyellow\"}\n",
    "sns.boxplot(x = 'Outcome', y = 'Glucose', data = df, palette = my_pal)\n",
    "plt.title('Glucose vs Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af514",
   "metadata": {},
   "source": [
    "Wow! the median of the Glucose level of Diabetic People is greater than the 75th Percentile of the glucose level of non-diabetic people. Therefore having a high glucose level does increase the chances of having diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64c9d6",
   "metadata": {},
   "source": [
    "#### Body Mass Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b4866",
   "metadata": {},
   "source": [
    "Body mass index (BMI) is a measure of body fat based on height and weight that applies to adult men and women. Does having a higher BMI leads to more chances of being diabetic? Let's check that out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pal = {0: \"lightyellow\", 1: \"lightpink\"}\n",
    "sns.boxplot(x = 'Outcome', y = 'BMI', data = df, palette = my_pal)\n",
    "plt.title('BMI vs Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150ed66",
   "metadata": {},
   "source": [
    "Indeed, the Median BMI of the Diabetic People is greater than the Median BMI of the Non-Diabetic people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae381f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of the data\n",
    "\n",
    "figure = plt.figure(figsize = (10, 10))\n",
    "corr_matrix = df.corr().round(2)\n",
    "sns.heatmap(data = corr_matrix, annot = True)\n",
    "plt.savefig(\"corrheatmap.jpg\")     #saving the image of this plot in the sagemaker console\n",
    "\n",
    "# The less correlation, the better. More correlation means presence of duplication of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64965b21",
   "metadata": {},
   "source": [
    "Almost all predictors have weak linear correlations, which is indicative that most of them are more likely to have non-linear relationships.\n",
    "\n",
    "However it is found that, the correlation between Pregnancies & Age is 54%, the correlation between SkinThickness & BMI is 39%, and the correlation between Insulin & SkinThickness is 44%.\n",
    "\n",
    "So, the population is advised to be concerned about the above issues in order to minimise the chances of diabetes.\n",
    "\n",
    "Further the analysis is mostly focused on the relationship between various diabetes features and the target feature which is diabetes outcome. This is because the classification purpose will be mostly interested in these types of correlation and their strengths in order for accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the image of this plot directly into the s3 bucket\n",
    "\n",
    "s3=boto3.resource('s3')\n",
    "s3.meta.client.upload_file('corrheatmap.jpg',bucket_name,'corrheatmap.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bede2",
   "metadata": {},
   "source": [
    "### Building and Deploying Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ea148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an output path where the trained model will be saved\n",
    "prefix = 'xgboost-as-a-built-in-algo'\n",
    "output_path ='s3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306566a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test split\n",
    "\n",
    "train_data, test_data = np.split(df.sample(frac=1, random_state=1729), [int(0.7 * len(df))])\n",
    "print(train_data.shape, test_data.shape)      #Not done as x_train y_train as we do in jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c5a86",
   "metadata": {},
   "source": [
    "* There are 537 rows and 9 columns in the train data.\n",
    "* There are 231 rows and 9 columns in the test data.\n",
    "\n",
    "While working in sagemaker the dependent feature that is 'Outcome' in this case should be the first column of the dataset.so, concatenating the train and test data in such a way that first column represents the dependent feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving Train And Test Into Buckets\n",
    "## We start with Train Data\n",
    "import os\n",
    "pd.concat([train_data['Outcome'], train_data.drop(['Outcome'], \n",
    "                                                axis=1)], \n",
    "                                                axis=1).to_csv('train.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')  #creating the path for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb023a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Into Buckets\n",
    "pd.concat([test_data['Outcome'], test_data.drop(['Outcome'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "s3_input_test = sagemaker.TrainingInput(s3_data='s3://{}/{}/test'.format(bucket_name, prefix), content_type='csv')   #creating the path for the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf2737c",
   "metadata": {},
   "source": [
    "#### Building Models Xgboot- Inbuilt Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name,    \n",
    "                          'xgboost', \n",
    "                          repo_version='1.0-1')     #Pulling the inbuilt xgboost container or image from sagemaker with recent repo_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "# The main purpose is to reduce the cost of model building\n",
    "\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":50\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62955891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "# shift +tab to undestand the estimator\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m5.2xlarge', #GPU version for speed\n",
    "                                          train_volume_size=5, # 5 GB \n",
    "                                          output_path=output_path,\n",
    "                                          train_use_spot_instances=True,\n",
    "                                          train_max_run=300,\n",
    "                                          train_max_wait=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "estimator.fit({'train': s3_input_train,'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5a605",
   "metadata": {},
   "source": [
    "The model has been created in the s3 bucket in the particular folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9358ed",
   "metadata": {},
   "source": [
    "#### Deploy Machine Learning Model As Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')   #endpoints will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb002ce",
   "metadata": {},
   "source": [
    "### Prediction on the Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29334d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer    #data is a csv file\n",
    "train_data_array = train_data.drop(['Outcome'], axis=1).values     #load the data into an array\n",
    "xgb_predictor.serializer = csv_serializer     # set the serializer type\n",
    "predictions_train = xgb_predictor.predict(train_data_array).decode('utf-8')   # predict!\n",
    "predictions_array_train = np.fromstring(predictions[1:], sep=',')   # and turn the prediction into an array\n",
    "print(predictions_array_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the confusion matrix on train data\n",
    "\n",
    "cm_train = pd.crosstab(index=train_data['Outcome'], columns=np.round(predictions_array_train), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn_train = cm_train.iloc[0,0]; fn_train = cm_train.iloc[1,0]; tp_train = cm_train.iloc[1,1]; fp_train = cm_train.iloc[0,1]; p_train = (tp_train+tn_train)/(tp_train+tn_train+fp_train+fn_train)*100\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p_train))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"Negative\", \"Positive\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"Negative\", tn_train/(tn_train+fn_train)*100,tn_train, fp_train/(tp_train+fp_train)*100, fp_train))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Positive\", fn_train/(tn_train+fn_train)*100,fn_train, tp_train/(tp_train+fp_train)*100, tp_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ebe27",
   "metadata": {},
   "source": [
    "### Prediction on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc86545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer    #data is a csv file\n",
    "test_data_array = test_data.drop(['Outcome'], axis=1).values     #load the data into an array\n",
    "xgb_predictor.serializer = csv_serializer     # set the serializer type\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8')   # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',')   # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the confusion matrix on test data\n",
    "\n",
    "cm = pd.crosstab(index=test_data['Outcome'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"Negative\", \"Positive\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"Negative\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Positive\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3981f71",
   "metadata": {},
   "source": [
    "### Deleting the endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc44b29",
   "metadata": {},
   "source": [
    "Once the prediction from the endpoint is done don't run it continuously because the charges will going on.\n",
    "once the endpoint address is created it needs to be deleted to avoid any extra charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85952d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)      #Deleteing the endpoint\n",
    "bucket_to_delete=boto3.resource('s3').Bucket(bucket_name)        #Deleting the bucket\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb5934",
   "metadata": {},
   "source": [
    "The process of deleting endpoints and s3 bucket can also be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a79ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
